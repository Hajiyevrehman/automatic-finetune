model:
  name: "Qwen/Qwen2.5-0.5B-Instruct"
  revision: "main"
  chat_template: "qwen-2.5"
  load_in_4bit: false

lora:
  r: 16
  alpha: 16
  dropout: 0
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"
  use_rslora: false

training:
  batch_size: 1
  gradient_accumulation_steps: 4
  num_epochs: 1
  learning_rate: 2.0e-4
  lr_scheduler: "cosine"
  weight_decay: 0.01
  warmup_ratio: 0.03
  use_gradient_checkpointing: "unsloth"
  fp16: true
  seed: 3407
  max_steps: 100
  save_steps: 50
  eval_steps: 50
  logging_steps: 10
  max_seq_length: 2048
  dataset_num_proc: 4
  packing: false
  train_on_responses_only: true
  optim: "paged_adamw_8bit"
  report_to: "none"
  test_after_training: true

dataset:
  from_s3: true
  train_file: "s3://llm-finetuning-rahman-1234/data/processed/servicenow-qa_tokenized_train.pt"
  validation_file: "s3://llm-finetuning-rahman-1234/data/processed/servicenow-qa_tokenized_val.pt"
  standardize_format: true

output:
  output_dir: "models/qwen-0.5b-finetuned"
  s3_output_path: "s3://llm-finetuning-rahman-1234/models/"

aws:
  access_key_id: ""
  secret_access_key: ""
  region: "us-east-1"