{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Unsloth LLM Finetuning for ServiceNow QA Dataset\n",
    "# \n",
    "# This notebook performs finetuning of Qwen2.5-7B on the ServiceNow QA dataset\n",
    "# using Unsloth for faster training.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path to import project modules\n",
    "project_root = Path.cwd().parent if \"notebooks\" in Path.cwd().parts else Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q unsloth boto3 peft\n",
    "!pip install -q --upgrade --no-cache-dir git+https://github.com/unslothai/unsloth.git\n",
    "\n",
    "# Import project modules\n",
    "from src.cloud.auth import get_s3_client\n",
    "from src.cloud.storage import download_from_s3, upload_to_s3\n",
    "\n",
    "# Load configuration\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "data_config_path = os.path.join(project_root, 'configs/data/data_processing.yaml')\n",
    "data_config = load_config(data_config_path)\n",
    "\n",
    "# Set up S3 connection\n",
    "s3_bucket = data_config['s3']['default_bucket']\n",
    "s3_region = data_config['s3']['region']\n",
    "\n",
    "# Create S3 client\n",
    "s3_client = get_s3_client(region=s3_region)\n",
    "\n",
    "# Download the ServiceNow QA dataset from S3\n",
    "local_data_path = \"servicenow-qa_converted.json\"\n",
    "s3_data_path = \"data/processed/servicenow-qa_converted.json\"\n",
    "\n",
    "print(f\"Downloading dataset from s3://{s3_bucket}/{s3_data_path}\")\n",
    "download_from_s3(s3_client, s3_bucket, s3_data_path, local_data_path)\n",
    "\n",
    "# Load the ServiceNow QA dataset\n",
    "with open(local_data_path, 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"Loaded dataset with {len(dataset)} examples\")\n",
    "\n",
    "# Set up Unsloth finetuning\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load training config\n",
    "training_config_path = os.path.join(project_root, 'configs/training/llm_finetuning.yaml')\n",
    "if os.path.exists(training_config_path):\n",
    "    training_config = load_config(training_config_path)\n",
    "else:\n",
    "    print(f\"Warning: Training config not found at {training_config_path}, using defaults\")\n",
    "    training_config = {\n",
    "        \"model\": {\n",
    "            \"name\": \"unsloth/Qwen2.5-7B\",\n",
    "            \"max_seq_length\": 2048,\n",
    "            \"load_in_4bit\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Get model parameters from config\n",
    "model_name = training_config['model']['name']\n",
    "max_seq_length = training_config['model']['max_seq_length']\n",
    "dtype = None  # Auto detection: Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = training_config['model']['load_in_4bit']\n",
    "\n",
    "# Log configuration being used\n",
    "print(f\"Using model configuration:\")\n",
    "print(f\"  - Model: {model_name}\")\n",
    "print(f\"  - Max Sequence Length: {max_seq_length}\")\n",
    "print(f\"  - Load in 4-bit: {load_in_4bit}\")\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading {model_name} with Unsloth optimization...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "\n",
    "# Get LoRA parameters from config\n",
    "lora_config = training_config.get('lora', {\n",
    "    \"r\": 16,\n",
    "    \"alpha\": 16,\n",
    "    \"dropout\": 0,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \"use_rslora\": False\n",
    "})\n",
    "\n",
    "# Log LoRA configuration\n",
    "print(f\"Using LoRA configuration:\")\n",
    "print(f\"  - Rank: {lora_config.get('r', 16)}\")\n",
    "print(f\"  - Alpha: {lora_config.get('alpha', 16)}\")\n",
    "print(f\"  - Dropout: {lora_config.get('dropout', 0)}\")\n",
    "print(f\"  - Target modules: {lora_config.get('target_modules')}\")\n",
    "print(f\"  - Using RS-LoRA: {lora_config.get('use_rslora', False)}\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "print(\"Adding LoRA adapters...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_config.get('r', 16),\n",
    "    target_modules=lora_config.get('target_modules'),\n",
    "    lora_alpha=lora_config.get('alpha', 16),\n",
    "    lora_dropout=lora_config.get('dropout', 0),\n",
    "    bias=\"none\",  # \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=lora_config.get('use_rslora', False),\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# Format the ServiceNow QA dataset for training\n",
    "from datasets import Dataset\n",
    "\n",
    "# Format dataset for training\n",
    "def format_servicenow_qa(examples):\n",
    "    formatted_examples = []\n",
    "    \n",
    "    for example in examples:\n",
    "        messages = example.get(\"messages\", [])\n",
    "        \n",
    "        # Extract system, user, and assistant messages\n",
    "        system_content = \"\"\n",
    "        user_content = \"\"\n",
    "        assistant_content = \"\"\n",
    "        \n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                system_content = message[\"content\"]\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                user_content = message[\"content\"]\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                assistant_content = message[\"content\"]\n",
    "        \n",
    "        # Create a formatted prompt using ChatML format\n",
    "        formatted_text = f\"\"\"<|im_start|>system\n",
    "{system_content}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_content}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{assistant_content}{tokenizer.eos_token}<|im_end|>\"\"\"\n",
    "        \n",
    "        formatted_examples.append({\"text\": formatted_text})\n",
    "    \n",
    "    return formatted_examples\n",
    "\n",
    "# Create HF dataset\n",
    "formatted_data = format_servicenow_qa(dataset)\n",
    "train_dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(f\"Formatted dataset with {len(train_dataset)} examples\")\n",
    "\n",
    "# Print a sample example to verify formatting\n",
    "print(\"\\nSample formatted example:\")\n",
    "print(train_dataset[0][\"text\"][:500] + \"...\\n\")\n",
    "\n",
    "# Set up TRL SFTTrainer\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Define output directory\n",
    "output_dir = \"finetuned_model\"\n",
    "\n",
    "# Get training parameters from config\n",
    "train_config = training_config.get('training', {\n",
    "    \"batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 5,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 3407,\n",
    "    \"logging_steps\": 10,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 3\n",
    "})\n",
    "\n",
    "# Get output directory from config\n",
    "output_config = training_config.get('output', {\n",
    "    \"dir\": \"models/finetuned\"\n",
    "})\n",
    "output_dir = output_config.get(\"dir\", \"finetuned_model\")\n",
    "\n",
    "# Log training configuration\n",
    "print(f\"Using training configuration:\")\n",
    "print(f\"  - Batch size: {train_config.get('batch_size', 2)}\")\n",
    "print(f\"  - Gradient accumulation steps: {train_config.get('gradient_accumulation_steps', 4)}\")\n",
    "print(f\"  - Number of epochs: {train_config.get('num_train_epochs', 3)}\")\n",
    "print(f\"  - Learning rate: {train_config.get('learning_rate', 2e-4)}\")\n",
    "print(f\"  - Output directory: {output_dir}\")\n",
    "\n",
    "# Create training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=train_config.get('batch_size', 2),\n",
    "    gradient_accumulation_steps=train_config.get('gradient_accumulation_steps', 4),\n",
    "    warmup_steps=train_config.get('warmup_steps', 5),\n",
    "    num_train_epochs=train_config.get('num_train_epochs', 3),\n",
    "    learning_rate=train_config.get('learning_rate', 2e-4),\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=train_config.get('logging_steps', 10),\n",
    "    optim=train_config.get('optim', \"adamw_8bit\"),\n",
    "    weight_decay=train_config.get('weight_decay', 0.01),\n",
    "    lr_scheduler_type=train_config.get('lr_scheduler_type', \"linear\"),\n",
    "    seed=train_config.get('seed', 3407),\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=train_config.get('evaluation_strategy', \"no\"),\n",
    "    save_strategy=train_config.get('save_strategy', \"epoch\"),\n",
    "    save_total_limit=train_config.get('save_total_limit', 3),\n",
    "    report_to=\"none\",  # Set to \"wandb\" if using Weights & Biases\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Display current memory stats\n",
    "print(\"\\nTraining Memory Stats:\")\n",
    "total_gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "allocated_gpu_memory = torch.cuda.memory_allocated() / (1024**3)\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Total GPU Memory: {total_gpu_memory:.3f} GB\")\n",
    "print(f\"Currently Allocated: {allocated_gpu_memory:.3f} GB\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show final memory and time stats\n",
    "training_time_seconds = trainer_stats.metrics.get(\"train_runtime\", 0)\n",
    "training_time_minutes = training_time_seconds / 60\n",
    "peak_memory_gb = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "\n",
    "print(f\"\\n{training_time_seconds:.4f} seconds used for training.\")\n",
    "print(f\"{training_time_minutes:.2f} minutes used for training.\")\n",
    "print(f\"Peak allocated memory = {peak_memory_gb:.3f} GB.\")\n",
    "print(f\"Peak allocated memory % of max memory = {(peak_memory_gb / total_gpu_memory) * 100:.3f} %.\")\n",
    "\n",
    "# Save the model\n",
    "print(\"\\nSaving model...\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test inference\n",
    "print(\"\\nTesting the finetuned model:\")\n",
    "test_question = \"How do I reset my ServiceNow password?\"\n",
    "\n",
    "# Format the test question using the ChatML format\n",
    "test_prompt = f\"\"\"<|im_start|>system\n",
    "You are a helpful AI assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{test_question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "print(\"\\nModel response:\")\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=200)\n",
    "\n",
    "# Get save configuration from config\n",
    "s3_config = training_config.get('s3', {\n",
    "    \"bucket\": s3_bucket,  # Use the one from data_config if not specified\n",
    "    \"model_prefix\": f\"models/{model_name.split('/')[-1]}-servicenow-qa\"\n",
    "})\n",
    "model_s3_path = s3_config.get(\"model_prefix\")\n",
    "\n",
    "# Get output formats from config\n",
    "output_formats = output_config.get(\"save_formats\", [\"lora\", \"gguf_q4_k_m\"])\n",
    "\n",
    "print(f\"\\nSaving model to S3:\")\n",
    "print(f\"  - S3 bucket: {s3_bucket}\")\n",
    "print(f\"  - S3 path: {model_s3_path}\")\n",
    "print(f\"  - Output formats: {output_formats}\")\n",
    "\n",
    "# Save LoRA adapters if specified\n",
    "if \"lora\" in output_formats:\n",
    "    print(\"\\nSaving LoRA adapters...\")\n",
    "    model.save_pretrained_merged(output_dir, tokenizer, save_method=\"lora\")\n",
    "    upload_to_s3(s3_client, s3_bucket, output_dir, f\"{model_s3_path}/lora\")\n",
    "\n",
    "# Save merged model if specified\n",
    "if \"merged_16bit\" in output_formats:\n",
    "    print(\"\\nSaving merged 16-bit model...\")\n",
    "    model.save_pretrained_merged(output_dir, tokenizer, save_method=\"merged_16bit\")\n",
    "    upload_to_s3(s3_client, s3_bucket, output_dir, f\"{model_s3_path}/merged_16bit\")\n",
    "\n",
    "# Save in 4-bit if specified\n",
    "if \"merged_4bit\" in output_formats:\n",
    "    print(\"\\nSaving merged 4-bit model...\")\n",
    "    model.save_pretrained_merged(output_dir, tokenizer, save_method=\"merged_4bit\")\n",
    "    upload_to_s3(s3_client, s3_bucket, output_dir, f\"{model_s3_path}/merged_4bit\")\n",
    "\n",
    "# Save in GGUF format if specified\n",
    "gguf_formats = [fmt for fmt in output_formats if fmt.startswith(\"gguf_\")]\n",
    "if gguf_formats:\n",
    "    print(\"\\nConverting to GGUF format(s)...\")\n",
    "    for gguf_format in gguf_formats:\n",
    "        # Extract quantization method from format name (e.g., \"gguf_q4_k_m\" -> \"q4_k_m\")\n",
    "        quant_method = gguf_format.replace(\"gguf_\", \"\")\n",
    "        print(f\"  - Creating GGUF with quantization: {quant_method}\")\n",
    "        model.save_pretrained_gguf(output_dir, tokenizer, quantization_method=quant_method)\n",
    "        upload_to_s3(\n",
    "            s3_client, \n",
    "            s3_bucket, \n",
    "            f\"{output_dir}-unsloth-{quant_method.upper()}.gguf\", \n",
    "            f\"{model_s3_path}/gguf/model-{quant_method}.gguf\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nModel successfully saved to s3://{s3_bucket}/{model_s3_path}/\")\n",
    "print(\"Finetuning complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
